{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class perceptron:\n",
    "    \"\"\"\n",
    "    A simple implementation of a multi-layer perceptron (MLP) neural network.\n",
    "\n",
    "    Attributes:\n",
    "    - n_layers (int): Number of layers in the neural network.\n",
    "    - n_neurons (list of int): Number of neurons in each layer.\n",
    "    - neurons (list of Layer): List containing the layers of the neural network.\n",
    "    - input (array): The input data for the forward pass.\n",
    "    - output (array): The output prediction of the neural network.\n",
    "\n",
    "    Methods:\n",
    "    - __init__(self, n_inputs, n_neurons, n_activation): Initializes the perceptron.\n",
    "    - forward(self, input): Performs the forward pass through the neural network.\n",
    "    - backward(self, error): Performs the backward pass to update the network's weights.\n",
    "    - train(self, x, Y, epochs): Trains the neural network on the given data.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_inputs, n_neurons, n_activation, eta = 1):\n",
    "        \"\"\"\n",
    "        Initializes the multi-layer perceptron with the specified architecture.\n",
    "\n",
    "        Args:\n",
    "        - n_inputs (int): Number of input features.\n",
    "        - n_neurons (list of int): Number of neurons in each layer.\n",
    "        - n_activation (list): List of activation functions for each layer.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Set the size of the network\n",
    "        self.n_layers = len(n_neurons)\n",
    "        self.n_neurons = n_neurons\n",
    "\n",
    "        # Define an array to hold all the layers\n",
    "        self.neurons = [Layer(n_inputs, n_neurons[0], n_activation[0], eta)]\n",
    "\n",
    "        # Define all hidden layers\n",
    "        for i in range(1, self.n_layers):\n",
    "            self.neurons.append(Layer(n_neurons[i-1], n_neurons[i], n_activation[i], eta))\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Performs the forward pass through the neural network.\n",
    "\n",
    "        Args:\n",
    "        - input (array): The input data for the forward pass.\n",
    "\n",
    "        Returns:\n",
    "        - output (array): The output prediction of the neural network.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Save the input\n",
    "        self.input = np.array(input)\n",
    "\n",
    "        # Pass the input to the first layer\n",
    "        self.neurons[0].forward(np.transpose(input))\n",
    "\n",
    "        # Iterate through all neurons\n",
    "        for i in range(1, self.n_layers):\n",
    "            self.neurons[i].forward(self.neurons[i-1].output)\n",
    "\n",
    "        # Set the MLP's output as the output of the last layer\n",
    "        self.output = self.neurons[-1].output\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, error):\n",
    "        \"\"\"\n",
    "        Performs the backward pass to update the network's weights.\n",
    "\n",
    "        Args:\n",
    "        - error (array): The error in the prediction.\n",
    "\n",
    "        Returns:\n",
    "        - gradients_prom (list of float): List of average gradients for each layer.\n",
    "\n",
    "        \"\"\"\n",
    "        #Initializing an array for the mean of the gradients in every layer\n",
    "        gradients_mean = [0] * self.n_layers\n",
    "\n",
    "        # Backpropagation for output layer\n",
    "        local_gradient_prev = self.neurons[-1].backward(-np.sum(error), 1)\n",
    "        gradients_mean[-1] = np.mean(local_gradient_prev)\n",
    "\n",
    "        # Backpropagation for all the other layers\n",
    "        for i in reversed(range(self.n_layers - 1)):\n",
    "            local_gradient_prev = self.neurons[i].backward(self.neurons[i + 1].weights, local_gradient_prev)\n",
    "            gradients_mean[i] = np.mean(local_gradient_prev)\n",
    "\n",
    "        return gradients_mean\n",
    "\n",
    "\n",
    "    def train(self, data, epochs, batch = False):\n",
    "        \"\"\"\n",
    "        Trains the neural network on the given data using backpropagation.\n",
    "\n",
    "        Args:\n",
    "        - data (array): Input data with the last value the target column.\n",
    "        - epochs (int): Number of training epochs.\n",
    "\n",
    "        Returns:\n",
    "        - gradients (list): List of gradients computed during training.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        #This part of the code cannot yet withstand multiple outputs\n",
    "        # Defining the train and test set\n",
    "        train, test, _ = train_test_val(data, (75,25,0))\n",
    "        train_x, train_y = train[:,0:-1], train[:,-1]\n",
    "        test_x, test_y = test[:,0:-1], test[:,-1]\n",
    "\n",
    "        # Check Y dimensions are of the size of the output layer\n",
    "        #assert len(train_y[0]) == self.n_neurons[-1]\n",
    "        \n",
    "        # Define auxiliary variables\n",
    "        data_len = len(train_x)\n",
    "        gradients = [0] * data_len * epochs;  gradient_epochs = [0]*epochs \n",
    "        instant_energy_train = [0] * data_len * epochs;  instant_average_energy_train = [0]*epochs \n",
    "        instant_average_energy_test = [0]*epochs \n",
    "        counter = 0 \n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            for i, input in enumerate(train_x):\n",
    "              y_pred = self.forward(np.array([input]))\n",
    "              error = np.array(train_y[i] - y_pred)\n",
    "              instant_energy_train[counter] = np.sum(error**2)/2\n",
    "              gradients[counter] = self.backward(error)\n",
    "              counter += 1\n",
    "            \n",
    "            # Epoch relevant information\n",
    "            gradient_epochs[epoch] = np.mean(gradients[epoch*data_len:epoch*data_len+data_len], axis = 0)\n",
    "            instant_average_energy_train[epoch] = np.mean(instant_energy_train[epoch*data_len:epoch*data_len+data_len])\n",
    "\n",
    "            #Test error\n",
    "            error_test = test_y - self.forward(test_x)\n",
    "            instant_average_energy_test[epoch] = np.mean(error_test**2)/2\n",
    "\n",
    "            #print(error_p.shape, test_y.shape, self.forward(np.array(test_x)).shape)\n",
    "              \n",
    "        return gradients, gradient_epochs, instant_energy_train, instant_average_energy_train, instant_average_energy_test\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Layer:\n",
    "    \"\"\"\n",
    "    A class representing a single layer in a neural network.\n",
    "\n",
    "    Attributes:\n",
    "    - weights (array): Weight matrix for the layer's connections.\n",
    "    - biases (array): Bias vector for the layer.\n",
    "    - activation (Activation): The activation function for the layer.\n",
    "    - stimuli (array): Input stimuli for the layer.\n",
    "    - field (array): Linear combination of stimuli, weights, and biases.\n",
    "    - output (array): Output of the layer after activation.\n",
    "    - local_gradient (array): Local gradient used in backpropagation.\n",
    "\n",
    "    Methods:\n",
    "    - __init__(self, n_inputs, n_neurons, activation): Initializes the layer.\n",
    "    - forward(self, stimuli): Performs the forward pass through the layer.\n",
    "    - backward(self, weights_prev, local_gradient_prev): Performs backpropagation for the layer.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_inputs, n_neurons, activation, eta=1):\n",
    "        \"\"\"\n",
    "        Initializes a neural network layer.\n",
    "\n",
    "        Args:\n",
    "        - n_inputs (int): Number of input features.\n",
    "        - n_neurons (int): Number of neurons in the layer.\n",
    "        - activation (Activation): Activation function for the layer.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialize weights with random values\n",
    "        self.weights = np.random.randn(n_neurons, n_inputs) * 2 - 1\n",
    "        self.biases = np.zeros((n_neurons, 1))  # Initialize biases with zeros\n",
    "        self.activation = activation()  # Create an instance of the provided activation function\n",
    "        # Learning rate (you can adjust this)\n",
    "        self.eta = eta\n",
    "\n",
    "    def forward(self, stimuli):\n",
    "        \"\"\"\n",
    "        Performs the forward pass through the layer.\n",
    "\n",
    "        Args:\n",
    "        - stimuli (array): The input stimuli for the layer.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Save the input stimuli for later use in backward pass\n",
    "        self.stimuli = stimuli\n",
    "\n",
    "        # Compute the initial linear combination of stimuli, weights, and biases\n",
    "        self.field = np.matmul(self.weights, self.stimuli) + self.biases\n",
    "\n",
    "        # Apply the activation function to the linear combination\n",
    "        self.output = self.activation.forward(self.field)\n",
    "\n",
    "    def backward(self, weights_prev, local_gradient_prev):\n",
    "        \"\"\"\n",
    "        Performs backpropagation for the layer.\n",
    "\n",
    "        Args:\n",
    "        - weights_prev (array): Weights from the next layer.\n",
    "        - local_gradient_prev (array): Local gradient from the next layer.\n",
    "\n",
    "        Returns:\n",
    "        - local_gradient (array): Local gradient for this layer.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        phi_prime = self.activation.backward(self.field)  # Compute the derivative of the activation function\n",
    "\n",
    "        # Compute local gradient for this layer using chain rule and weights from the next layer\n",
    "        self.local_gradient = np.multiply(phi_prime, np.dot(weights_prev.T, local_gradient_prev))\n",
    "\n",
    "        # Compute weight update using the local gradient and input stimuli\n",
    "        delta_weights = np.matmul(self.local_gradient, self.stimuli.T)\n",
    "        assert delta_weights.shape == self.weights.shape\n",
    "\n",
    "        # Update weights using the learning rate and calculated delta\n",
    "        self.weights = self.weights + self.eta * delta_weights\n",
    "\n",
    "        # Update biases using the learning rate and the local gradient\n",
    "        delta_biases =  -self.eta * np.mean(self.local_gradient, axis=1, keepdims=True)\n",
    "        assert delta_biases.shape == self.biases.shape\n",
    "        self.biases = self.biases + self.eta *delta_biases\n",
    "\n",
    "        return self.local_gradient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sigmoid :\n",
    "  def forward(self, input):\n",
    "    self.output = 1/(1 + np.exp(-input))\n",
    "    return self.output\n",
    "\n",
    "  def backward(self, output):\n",
    "    self.back = np.exp(-output)/(np.exp(-output) + 1)**2\n",
    "    return self.back\n",
    "\n",
    "class tanh :\n",
    "  def forward(self, input):\n",
    "    return np.tanh(input)\n",
    "\n",
    "  def backward(self,output):\n",
    "    self.back = 1 - np.tanh(output)**2\n",
    "    return self.back\n",
    "\n",
    "class linear:\n",
    "  def forward(self, input):\n",
    "    self.a = 1\n",
    "    self.b = 0\n",
    "    return np.dot(self.a,input) +self.b\n",
    "\n",
    "  def backward(self, output):\n",
    "    length = int(len(output))\n",
    "    return np.array([[self.a]]*length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Manipulations\n",
    "\n",
    "def train_test_val(data, sizes = (60,20,20)):\n",
    "  sizes = [x/100 for x in sizes]\n",
    "  length = len(data)\n",
    "  nonTrainLen = int(length*(sizes[1]+sizes[2]))\n",
    "\n",
    "  idx = np.random.choice(np.arange(0,length), size = nonTrainLen, replace=False)\n",
    "  assert len(idx) == nonTrainLen\n",
    "\n",
    "  idxTrain = np.setdiff1d(np.arange(0,length), idx)\n",
    "  assert len(idx) == nonTrainLen\n",
    "\n",
    "  testLen = int(nonTrainLen*sizes[1]/(sizes[1]+sizes[2]))\n",
    "  idxTest = np.random.choice(np.arange(0,len(idx)), size = testLen, replace=False)\n",
    "  assert len(idxTest) == testLen\n",
    "\n",
    "  testVal = int(nonTrainLen*sizes[2]/(sizes[1]+sizes[2]))\n",
    "  idxValid = np.setdiff1d(np.arange(0,nonTrainLen), idxTest)\n",
    "  assert len(idxValid) == testVal\n",
    "\n",
    "  assert(list(np.intersect1d(idxValid, idxTest)) == [])\n",
    "  assert(list(np.intersect1d(idx, idxTrain)) == [])\n",
    "  train = np.array([data[i] for i in idxTrain])\n",
    "  test, valid = np.array([data[idx[i]] for i in idxTest]), np.array([data[idx[i]] for i in idxValid])\n",
    "  return train, test, valid\n",
    "\n",
    "def normalize_min_max(matrix):\n",
    "  min_val = np.min(matrix)\n",
    "  max_val = np.max(matrix)\n",
    "  normalized_matrix = (matrix - min_val) / (max_val - min_val)\n",
    "  return normalized_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DatosIA.mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "\n",
    "mat = scipy.io.loadmat('datosIA.mat')\n",
    "\n",
    "data = np.column_stack([mat[\"X\"], mat[\"OD\"], mat[\"S\"]])\n",
    "data = normalize_min_max(data)\n",
    "\n",
    "# X = [X, OD], Y = [S]\n",
    "train, _, valid = train_test_val(data, (80,0,20))\n",
    "\n",
    "n_neurons = [2,1]\n",
    "n_activation = [sigmoid]*len(n_neurons)\n",
    "\n",
    "MLP_1CapaOculta = perceptron(2,n_neurons, n_activation,0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradients, gradient_epochs, instant_energy_train, instant_average_energy_train, instant_average_energy_test = MLP_1CapaOculta.train(train,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.05041685984414076,\n",
       " 0.051851222159427564,\n",
       " 0.05295970502774779,\n",
       " 0.053944788521051515,\n",
       " 0.05493313962478981,\n",
       " 0.055968773751773594,\n",
       " 0.056997976548206736,\n",
       " 0.05789069380547213,\n",
       " 0.05852028637277819,\n",
       " 0.058845636115228056,\n",
       " 0.05892146006771794,\n",
       " 0.058845254262945,\n",
       " 0.058703809532889215,\n",
       " 0.05855222129115055,\n",
       " 0.05841721126812222,\n",
       " 0.058307726532040305,\n",
       " 0.05822384298999303,\n",
       " 0.05816212970354628,\n",
       " 0.058118321140156026,\n",
       " 0.05808844241657907]"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instant_average_energy_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 5, 3, 3]"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[1] + [2,3,5,3] + [3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
