{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MLP import *\n",
    "from dataManipulation import *\n",
    "from activationFunctions import *\n",
    "import MLP_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from activationFunctions import *\n",
    "from dataManipulation import * \n",
    "\n",
    "class Perceptron(object):\n",
    "    def __init__(self,  num_inputs: int, num_neurons: list, num_outputs: int, func_activation: list, eta = 0.01) -> None:\n",
    "        \"\"\" \n",
    "        Initializes the multi-layer perceptron with the specified architecture.\n",
    "\n",
    "        Args:\n",
    "        - num_inputs (int): Number of input features.\n",
    "        - num_neurons (list of int): Number of neurons in each layer.\n",
    "        - num_output (int): Number of output features.\n",
    "        - func_activation (list): List of activation functions for each layer.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Set the size of the network\n",
    "        n_layers = [num_inputs] + num_neurons + [num_outputs]\n",
    "        self.num_layers = len(n_layers)\n",
    "        self.layers = [0]*self.num_layers\n",
    "        self.num_neurons = n_layers\n",
    "        \n",
    "\n",
    "        # Define all of the layers\n",
    "        input = num_inputs\n",
    "        for i, neuron in enumerate(n_layers):\n",
    "            self.layers[i] = Layer(input, neuron, func_activation[i], eta)\n",
    "            input = neuron\n",
    "\n",
    "    def forward(self, input: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Performs the forward pass through the neural network.\n",
    "\n",
    "        Args:\n",
    "        - input (array): The input data for the forward pass.\n",
    "\n",
    "        Returns:\n",
    "        - output (array): The output prediction of the neural network.\n",
    "\n",
    "        \"\"\"\n",
    "        input = input.reshape(-1,1)\n",
    "        # Iterate through all neurons\n",
    "        for i in range(self.num_layers):\n",
    "            input = self.layers[i].forward(input)\n",
    "\n",
    "        self.output = input\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, error: np.ndarray) -> list:\n",
    "        \"\"\"\n",
    "        Performs the backward pass to update the network's weights.\n",
    "\n",
    "        Args:\n",
    "        - error (array): The error in the prediction.\n",
    "\n",
    "        Returns:\n",
    "        - gradients_prom (list of float): List of average gradients for each layer.\n",
    "\n",
    "        \"\"\"\n",
    "        #Initializing an array for the mean of the gradients in every layer\n",
    "        gradients = [0] * self.num_layers\n",
    "\n",
    "        # Backpropagation for output layer\n",
    "        gradients[-1] = self.layers[-1].backward(-np.sum(error), 1, True)\n",
    "        \n",
    "        # Backpropagation for all the other layers\n",
    "        for i in reversed(range(self.num_layers - 1)):\n",
    "            gradients[i] = self.layers[i].backward(self.layers[i + 1].weights, gradients[i+1])\n",
    "\n",
    "        return gradients\n",
    "\n",
    "    def train_batch(self, data, epochs=50, iterations=10):\n",
    "        \"\"\"\n",
    "        Train the neural network using mini-batch gradient descent.\n",
    "\n",
    "        Args:\n",
    "        - data (array): Input data with the last value as the target column.\n",
    "        - epochs (int): Number of training epochs.\n",
    "        - iterations (int): Number of mini-batch iterations per epoch.\n",
    "\n",
    "        Returns:\n",
    "        - gradients (list): List of gradients computed during training.\n",
    "        - gradient_epochs (list): List of average gradients for each epoch.\n",
    "        - instant_energy_train (list): List of instant energy values during training.\n",
    "        - instant_average_energy_train (list): List of average energy values during training.\n",
    "        - instant_average_energy_test (list): List of average energy values during testing.\n",
    "\n",
    "        \"\"\"\n",
    "        # Split data into training and testing sets\n",
    "        train, test, _ = train_test_val(data, (75, 25, 0))\n",
    "\n",
    "        # Initialize counters and auxiliary variables\n",
    "        counter = 0\n",
    "        data_len = len(train)\n",
    "        gradients = [0] * iterations * epochs;   gradient_epochs = [0] * iterations\n",
    "        instant_energy_train = [0] * epochs * iterations;  instant_average_energy_train = [0] * iterations\n",
    "        instant_average_energy_test = [0] * epochs * iterations\n",
    "\n",
    "        \n",
    "        # Split the training data into input and target\n",
    "        train_x, train_y = train[:, 0:-1], train[:, -1]\n",
    "        test_x, test_y = test[:, 0:-1], test[:, -1]\n",
    "\n",
    "        for iter in range(iterations):\n",
    "\n",
    "            for epoch in range(epochs):\n",
    "                # Forward pass for the entire training set\n",
    "                y_pred = np.array([self.forward(x) for x in train_x])\n",
    "                error = train_y - y_pred\n",
    "\n",
    "                # Find the index with the largest error\n",
    "                idx = np.argmax(error)\n",
    "\n",
    "                # Compute instant energy and gradients for the mini-batch\n",
    "                instant_energy_train[counter] = np.mean(error**2) / 2\n",
    "                self.forward(np.array([train_x[idx, :]]))\n",
    "                gradients[counter] = self.backward(error[idx])\n",
    "\n",
    "                # Test error\n",
    "                error_test = test_y - np.array([self.forward(x) for x in test_x])\n",
    "                instant_average_energy_test[counter] = np.mean(error_test**2) / 2\n",
    "\n",
    "                # Early stopping if test error is below a threshold\n",
    "                if instant_average_energy_test[counter] < 0.02:\n",
    "                    print(\"Paroooo\")\n",
    "                    counter += epochs-epoch-1\n",
    "                    break\n",
    "                elif instant_average_energy_test[counter] > instant_energy_train[counter] +0.5 :\n",
    "                    # If the testing error is considerably higher than the training one, resample\n",
    "                    print(\"Resampleooo\")\n",
    "                    train_x, train_y = train[:, 0:-1], train[:, -1]\n",
    "                    test_x, test_y = test[:, 0:-1], test[:, -1]\n",
    "                counter += 1\n",
    "\n",
    "            # Compute average gradient and average energy for the current iteration\n",
    "            #gradient_epochs[iter] = np.mean(gradients[iter * epochs:iter * epochs + epoch +1], axis=0)\n",
    "            instant_average_energy_train[iter] = np.mean(instant_energy_train[iter * epochs:iter * epochs + epoch+1])\n",
    "\n",
    "        return gradients, gradient_epochs, instant_energy_train, instant_average_energy_train, instant_average_energy_test\n",
    "\n",
    "\n",
    "class Layer():\n",
    "    def __init__(self, num_inputs: int, num_neurons:int, activation: object, eta:float) -> None:\n",
    "        \"\"\"\n",
    "        Initializes a neural network layer.\n",
    "\n",
    "        Args:\n",
    "        - n_inputs (int): Number of input features.\n",
    "        - n_neurons (int): Number of neurons in the layer.\n",
    "        - activation (Activation): Activation function for the layer.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialize weights randomly (you can uncomment the random initialization below)\n",
    "        #self.weights = np.random.randn(num_neurons, num_inputs) * 2 - 1\n",
    "        self.weights = np.ones((num_neurons, num_inputs))\n",
    "\n",
    "        # Initialize bias randomly (you can uncomment the random initialization below)\n",
    "        #self.bias = np.random.randn(num_neurons, 1) * 2 - 1\n",
    "        self.bias = np.ones((num_neurons, 1))\n",
    "        self.weights = np.hstack((self.weights, self.bias))\n",
    "        \n",
    "        # Set Activation function\n",
    "        self.activation = activation  # Create an instance of the provided activation function\n",
    "        # Learning rate (you can adjust this)\n",
    "        self.eta = eta\n",
    "\n",
    "    def forward(self, input: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Performs the forward pass through the layer.\n",
    "\n",
    "        Args:\n",
    "        - input (array): The input stimuli for the layer.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Save the input input for later use in backward pass\n",
    "        self.input = input\n",
    "\n",
    "        # Compute the initial linear combination of input and weights\n",
    "        self.field = np.matmul(self.weights, np.vstack((self.input,1)))\n",
    "\n",
    "        # Apply the activation function to the linear combination\n",
    "        self.output = self.activation.forward(self.field)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, weights_prev, local_gradient_prev, first = False):\n",
    "        \"\"\"\n",
    "        Performs backpropagation for the layer.\n",
    "\n",
    "        Args:\n",
    "        - weights_prev (array): Weights from the next layer.\n",
    "        - local_gradient_prev (array): Local gradient from the next layer.\n",
    "\n",
    "        Returns:\n",
    "        - local_gradient (array): Local gradient for this layer.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        phi_prime = self.activation.backward(self.field)  # Compute the derivative of the activation function\n",
    "\n",
    "        # Compute local gradient for this layer using chain rule and weights from the next layer\n",
    "        local_gradient = np.multiply(phi_prime, np.dot(weights_prev[:,0:-1].T if not first else weights_prev, local_gradient_prev))\n",
    "\n",
    "        # Compute weight update using the local gradient and input stimuli\n",
    "        delta = np.dot(local_gradient, np.vstack((self.input,1)).T) # Weight and bias change\n",
    "        assert delta.shape == self.weights.shape\n",
    "\n",
    "        # Update weights using the learning rate and calculated delta\n",
    "        weights_new = self.weights + self.eta * delta\n",
    "        self.weights = weights_new\n",
    "\n",
    "        return local_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "mat = scipy.io.loadmat('datosIA.mat')\n",
    "\n",
    "data = np.column_stack([mat[\"X\"], mat[\"OD\"], mat[\"S\"]])\n",
    "data = normalize_min_max(data)\n",
    "\n",
    "# X = [X, OD], Y = [S]\n",
    "train, _, valid = train_test_val(data, (80,0,20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neurons = [3]\n",
    "n_activation = [linear(0.3), sigmoid(1.5), tanh(0.3)]\n",
    "\n",
    "MLP = Perceptron(2,n_neurons, 1, n_activation,0.3)\n",
    "grad, grad_epoc, inst_ener_train, inst_aver_ener_train, inst_aver_ener_test = MLP.train_batch(train,50,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.39522597750728033\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(np.mean(inst_aver_ener_test))\n",
    "np.array([MLP.forward(x) for x in valid[:,0:-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 42.07537083,  42.07537083, 136.28736133],\n",
       "       [ 42.07537083,  42.07537083, 136.28736133],\n",
       "       [ 42.07537083,  42.07537083, 136.28736133]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MLP.layers[1].weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = scipy.io.loadmat('datosIA.mat')\n",
    "\n",
    "data = np.column_stack([mat[\"X\"], mat[\"OD\"], mat[\"S\"]])\n",
    "data = normalize_min_max(data)\n",
    "\n",
    "# X = [X, OD], Y = [S]\n",
    "train, _, valid = train_test_val(data, (80,0,20))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
